{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from maze_env import Maze\n",
    "from utility import RL\n",
    "from RL_control import DeepQNetwork\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def onKeyPress(event):\n",
    "#     is_terminate = False\n",
    "#     if (event.char == \"w\"):\n",
    "#         is_terminate, reward = env.take_action(0, show_animate=True)\n",
    "#     elif (event.char == \"d\"):\n",
    "#         is_terminate, reward = env.take_action(1, show_animate=True)\n",
    "#     elif (event.char == \"s\"):\n",
    "#         is_terminate, reward = env.take_action(2, show_animate=True)\n",
    "#     elif (event.char == \"a\"):\n",
    "#         is_terminate, reward = env.take_action(3, show_animate=True)\n",
    "#     elif (event.char == \"q\"):\n",
    "#         env.destroy()\n",
    "#     if (is_terminate):\n",
    "#         env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4x4, act_space = 4\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "env = Maze()\n",
    "rl = RL(env)\n",
    "print('{0}x{1}, act_space = {2}'.format(env.MAZE_Limit[0],env.MAZE_Limit[1], len(env.action_space)))\n",
    "# env.bind('<KeyPress>', onKeyPress)\n",
    "# env.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 [[ 0. -1. -2. -3.]\n",
      " [-1. -2.  0. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "rl.DP.iteration(update = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is MC\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 3000,  model = \"MC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -5.20420479 -7.01868919 -7.52032503]\n",
      " [-5.13818873 -6.25287097  0.         -7.19502524]\n",
      " [-6.98408265 -6.94825781 -6.26065077 -5.19816851]\n",
      " [-7.50945651 -7.00552472 -5.08479278  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -5.75754682 -7.1063444 ]\n",
      " [-1.         -5.42449402  0.         -5.79526423]\n",
      " [-5.71122675 -6.56020825 -5.46707899 -1.        ]\n",
      " [-7.14422829 -5.55900961 -1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "final_policy = rl.MF.policy\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[0],env.MAZE_Limit[1]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[0],env.MAZE_Limit[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free TD(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "rl.MF.gamma = 0.9 # discount factor\n",
    "rl.MF.lamb = 0.9 # parameter of sarsa\n",
    "rl.MF.alpha = 0.5 # step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_0\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 3000,  model = \"TD_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -4.44343194 -7.7653562  -8.30400122]\n",
      " [-5.05419457 -5.60130001  0.         -7.27974744]\n",
      " [-6.37378648 -6.85965699 -5.2636192  -6.17026273]\n",
      " [-7.61081033 -6.37501865 -5.28514332  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -6.77619063 -7.63120865]\n",
      " [-1.         -4.19153775  0.         -4.76980564]\n",
      " [-2.94873434 -6.00407332 -3.31299486 -1.        ]\n",
      " [-7.11771024 -4.07053033 -1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free TD(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_n\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 3000,  model = \"TD_n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -3.75487437 -5.96489609 -7.41857246]\n",
      " [-4.37537882 -6.00092508  0.         -6.78123281]\n",
      " [-6.08546948 -6.25416345 -5.96758898 -4.60258688]\n",
      " [-8.19986119 -7.38832561 -5.54994619  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -3.85543615 -6.63699061]\n",
      " [-1.         -3.92881665  0.         -6.23330791]\n",
      " [-3.05180604 -5.15777862 -4.79715167 -1.        ]\n",
      " [-7.11870367 -6.08874361 -1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free TD(lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_lambda\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 1000,  model = \"TD_lambda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -5.94479937 -6.70113288 -7.15159729]\n",
      " [-5.03796453 -6.273026    0.         -4.95964602]\n",
      " [-8.25236142 -6.78051515 -5.19864859 -4.41129735]\n",
      " [-8.66545659 -7.57850982 -5.00955175  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -5.06286602 -6.56322458]\n",
      " [-1.         -3.94060371  0.         -3.88573462]\n",
      " [-7.67062149 -4.07904985 -2.33618171 -1.        ]\n",
      " [-8.24883766 -6.25042731 -1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free Control MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is MC\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 3000,  model = \"MC\", control = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -1.96949322 -3.4829283  -4.65396878]\n",
      " [-1.94120169 -3.09467254  0.         -3.36864278]\n",
      " [-3.33318475 -3.85356214 -3.12975857 -1.90335529]\n",
      " [-4.20162906 -3.3457591  -1.95654087  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -2.78740907 -4.092357  ]\n",
      " [-1.         -2.74750745  0.         -2.68337168]\n",
      " [-2.63797238 -3.79768777 -2.78755798 -1.        ]\n",
      " [-3.93036833 -2.72540429 -1.          0.        ]]\n",
      "Policy\n",
      " [[0 3 3 3]\n",
      " [0 3 0 2]\n",
      " [0 0 1 2]\n",
      " [1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "final_policy = rl.MF.policy\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"Policy\\n\",np.argmax(final_policy,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free Control Sarsa(lambda) (Q-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_lambda\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 30000,  model = \"TD_lambda\", control = True, on_policy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -1.85065915 -3.76117155 -3.65755408]\n",
      " [-1.91980625 -3.20157548  0.         -3.41873487]\n",
      " [-3.84179095 -5.26992457 -4.02811553 -1.86745719]\n",
      " [-5.67116843 -4.02588911 -2.39803986  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -2.45462739 -3.07650438]\n",
      " [-1.         -2.52488652  0.         -2.07477999]\n",
      " [-2.54262205 -5.08229124 -3.54156819 -1.        ]\n",
      " [-5.33139205 -3.17478601 -1.          0.        ]]\n",
      "Policy\n",
      " [[0 3 3 2]\n",
      " [0 3 0 2]\n",
      " [0 2 1 2]\n",
      " [0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "final_policy = rl.MF.policy\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"Policy\\n\",np.argmax(final_policy,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "a = np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  4.,  8., 16.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t = time.time()\n",
    "np.logspace(start=1,stop=len(a),num=len(a),endpoint=True,base=2)/2\n",
    "# print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  4,  8, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t = time.time()\n",
    "np.array([2**i for i in range(len(a))])\n",
    "# print(time.time()-t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

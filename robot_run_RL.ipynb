{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from maze_env import Maze\n",
    "from rl_algo import RL\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def onKeyPress(event):\n",
    "#     is_terminate = False\n",
    "#     if (event.char == \"w\"):\n",
    "#         is_terminate, reward = env.take_action(0, show_animate=True)\n",
    "#     elif (event.char == \"d\"):\n",
    "#         is_terminate, reward = env.take_action(1, show_animate=True)\n",
    "#     elif (event.char == \"s\"):\n",
    "#         is_terminate, reward = env.take_action(2, show_animate=True)\n",
    "#     elif (event.char == \"a\"):\n",
    "#         is_terminate, reward = env.take_action(3, show_animate=True)\n",
    "#     elif (event.char == \"q\"):\n",
    "#         env.destroy()\n",
    "#     if (is_terminate):\n",
    "#         env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4x4, act_space = 4\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "env = Maze()\n",
    "rl = RL(env)\n",
    "print('{0}x{1}, act_space = {2}'.format(env.MAZE_Limit[0],env.MAZE_Limit[1], len(env.action_space)))\n",
    "# env.bind('<KeyPress>', onKeyPress)\n",
    "# env.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 [[ 0.         -6.13796997 -8.35235596 -8.96731567]\n",
      " [-6.13796997 -7.73739624 -8.42782593 -8.35235596]\n",
      " [-8.35235596 -8.42782593 -7.73739624 -6.13796997]\n",
      " [-8.96731567 -8.35235596 -6.13796997  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "rl.DP.iteration(update = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is MC\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 3000,  model = \"MC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -5.27053431 -7.08243896 -7.53820698]\n",
      " [-5.20003158 -6.58591537 -7.15160434 -7.0463178 ]\n",
      " [-7.11696545 -7.15369314 -6.56095182 -5.28408376]\n",
      " [-7.66945791 -7.14699407 -5.37471094  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -5.71425624 -7.20560191]\n",
      " [-1.         -5.69472813 -6.84765479 -5.66332242]\n",
      " [-5.68230871 -6.85394435 -5.75995984 -1.        ]\n",
      " [-7.4177236  -5.88827504 -1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "final_policy = rl.MF.policy\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[0],env.MAZE_Limit[1]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[0],env.MAZE_Limit[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State-value\n",
      " [[ 0.         -5.19944911 -7.13899785 -7.54970029]\n",
      " [-5.26398961 -6.61190632 -7.15188055 -7.0409637 ]\n",
      " [-7.14641922 -7.17062527 -6.56812609 -5.21796383]\n",
      " [-7.69311891 -7.153942   -5.34481712  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "value = rl.MF.value\n",
    "print(\"State-value\\n\",value.reshape(env.MAZE_Limit[0],env.MAZE_Limit[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free TD(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "rl.MF.gamma = 0.9 # discount factor\n",
    "rl.MF.lamb = 0.9 # parameter of sarsa\n",
    "rl.MF.alpha = 0.5 # step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_0\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 3000,  model = \"TD_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -5.54602796 -7.46634413 -7.3993604 ]\n",
      " [-5.06445209 -6.33292785 -6.59121497 -7.74911209]\n",
      " [-7.19309727 -7.15905243 -7.43564025 -4.93107317]\n",
      " [-8.15530865 -7.19775891 -5.68332405  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -6.82001725 -6.78757865]\n",
      " [-1.         -3.93156081 -5.75316368 -7.02480688]\n",
      " [-4.99013165 -5.99491964 -6.67040985 -1.        ]\n",
      " [-7.96031471 -4.77746127 -1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free TD(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_n\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 3000,  model = \"TD_n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -6.14636691 -7.94613537 -6.93068934]\n",
      " [-5.52952261 -7.62071206 -7.12039156 -5.80201448]\n",
      " [-6.65193122 -7.65362405 -7.19087101 -6.00588349]\n",
      " [-7.0211792  -5.91757094 -5.49191165  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -7.50073197 -5.52801824]\n",
      " [-1.         -6.35942268 -4.3121796  -4.60320922]\n",
      " [-4.83939849 -7.28258755 -5.66689102 -1.        ]\n",
      " [-6.00455494 -4.21807797 -1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free TD(lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_lambda\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 1000,  model = \"TD_lambda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -3.69946357 -4.76371773 -5.46002914]\n",
      " [-4.74756823 -6.22196233 -5.47818761 -6.36814757]\n",
      " [-5.64043172 -7.13414864 -5.62027456 -6.09342144]\n",
      " [-7.2822621  -7.37045521 -5.79312181  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -2.95620667 -3.61683393]\n",
      " [-1.         -2.88923652 -4.03253364 -4.63197559]\n",
      " [-4.43884214 -5.71887618 -2.39576074 -1.        ]\n",
      " [-6.79849794 -5.41051596 -1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free Control MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is MC\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 1000,  model = \"MC\", control = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -2.17825844 -4.38241685 -5.051571  ]\n",
      " [-1.95404042 -3.25709775 -4.25658532 -3.56222046]\n",
      " [-3.35798839 -4.07881956 -3.51287489 -2.13725835]\n",
      " [-4.27414389 -3.44629846 -2.00022044  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -3.54918616 -4.37086627]\n",
      " [-1.         -2.75509057 -4.09805844 -2.79547906]\n",
      " [-2.64837258 -3.97672804 -2.97106789 -1.        ]\n",
      " [-3.7359354  -2.85124183 -1.          0.        ]]\n",
      "Policy\n",
      " [[0 3 3 2]\n",
      " [0 3 1 2]\n",
      " [0 1 2 2]\n",
      " [0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "final_policy = rl.MF.policy\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"Policy\\n\",np.argmax(final_policy,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free Control Sarsa(lambda) (Q-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_lambda\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 1000,  model = \"TD_lambda\", control = True, on_policy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -1.95711912 -3.70996963 -3.79384564]\n",
      " [-1.92225595 -3.11268798 -3.44803186 -3.28383822]\n",
      " [-3.78907641 -4.1769261  -2.8411653  -1.93819066]\n",
      " [-3.74124002 -3.00138888 -2.02957962  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -3.30362906 -2.96845358]\n",
      " [-1.         -2.52983703 -2.97557706 -2.68704847]\n",
      " [-2.84699987 -3.81821767 -2.18623878 -1.        ]\n",
      " [-2.9378979  -2.01585406 -1.          0.        ]]\n",
      "Policy\n",
      " [[0 3 3 2]\n",
      " [0 0 3 2]\n",
      " [0 3 2 2]\n",
      " [1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "final_policy = rl.MF.policy\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"Policy\\n\",np.argmax(final_policy,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

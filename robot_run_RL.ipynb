{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from maze_env import Maze\n",
    "from rl_algo import RL\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def onKeyPress(event):\n",
    "#     is_terminate = False\n",
    "#     if (event.char == \"w\"):\n",
    "#         is_terminate, reward = env.take_action(0, show_animate=True)\n",
    "#     elif (event.char == \"d\"):\n",
    "#         is_terminate, reward = env.take_action(1, show_animate=True)\n",
    "#     elif (event.char == \"s\"):\n",
    "#         is_terminate, reward = env.take_action(2, show_animate=True)\n",
    "#     elif (event.char == \"a\"):\n",
    "#         is_terminate, reward = env.take_action(3, show_animate=True)\n",
    "#     elif (event.char == \"q\"):\n",
    "#         env.destroy()\n",
    "#     if (is_terminate):\n",
    "#         env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4x4, act_space = 4\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "env = Maze()\n",
    "rl = RL(env)\n",
    "print('{0}x{1}, act_space = {2}'.format(env.MAZE_Limit[0],env.MAZE_Limit[1], len(env.action_space)))\n",
    "# env.bind('<KeyPress>', onKeyPress)\n",
    "# env.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 [[10. 10. 10. 10.]\n",
      " [10. 10. 10. 10.]\n",
      " [10.  0. 10. 10.]\n",
      " [10. 10. 10. 10.]]\n"
     ]
    }
   ],
   "source": [
    "rl.DP.iteration(update = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is MC\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 3000,  model = \"MC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[0.62585038 0.78608358 1.11887695 1.40881278]\n",
      " [0.71809319 0.94859475 1.74725861 2.40958605]\n",
      " [0.88380037 0.         3.35059863 4.87514551]\n",
      " [1.43519173 2.63662704 5.03536485 0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.74486238  1.01670766  1.57809469  2.08595458]\n",
      " [ 0.88882886  1.64393913  2.99308136  4.62812882]\n",
      " [ 1.31190107  0.          4.37804437 10.        ]\n",
      " [ 2.38090278  4.48191235 10.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "final_policy = rl.MF.policy\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[0],env.MAZE_Limit[1]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[0],env.MAZE_Limit[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free TD(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "rl.MF.gamma = 0.9 # discount factor\n",
    "rl.MF.lamb = 0.9 # parameter of sarsa\n",
    "rl.MF.alpha = 0.5 # step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_0\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 3000,  model = \"TD_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[0.53831762 0.5930581  1.07821956 1.61749789]\n",
      " [0.68039485 0.75881308 1.7823823  2.63708201]\n",
      " [0.61759477 0.         4.22406872 4.17001913]\n",
      " [1.27088779 3.83802703 6.10709242 0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.60016359  0.73449873  2.04121138  3.38715763]\n",
      " [ 0.79580631  1.15641876  4.09757531  6.40276973]\n",
      " [ 0.82562708  0.          7.00166838 10.        ]\n",
      " [ 1.75170987  6.93635633 10.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free TD(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_n\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 3000,  model = \"TD_n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[0.38981509 1.13580878 1.38239575 0.7444338 ]\n",
      " [0.75308717 1.68967572 2.61253151 0.93474725]\n",
      " [0.64518846 0.         4.30474857 4.66936251]\n",
      " [1.38391211 1.74190399 5.36915781 0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.59742621  1.93232227  3.05994302  2.13102189]\n",
      " [ 2.04032362  5.08028168  6.10415251  1.6254683 ]\n",
      " [ 1.20857975  0.          7.88567047 10.        ]\n",
      " [ 3.16214294  2.95506125 10.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free TD(lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_lambda\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 1000,  model = \"TD_lambda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[0.38272982 0.55181252 0.97123525 0.81562638]\n",
      " [0.60358107 0.62177802 1.82873997 2.51663787]\n",
      " [0.67633124 0.         3.27005579 6.92154598]\n",
      " [1.03422086 2.22300168 5.3591418  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.46806593  0.63559816  2.40805121  1.38313305]\n",
      " [ 0.95594241  0.79426336  4.57962842  3.45591471]\n",
      " [ 1.11137687  0.          8.04985471 10.54053963]\n",
      " [ 1.60352294  3.74264007 10.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free Control MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is MC\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 1000,  model = \"MC\", control = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[3.84079131 4.42944661 5.25532944 5.7898373 ]\n",
      " [4.23698361 5.0311316  6.17753204 7.09227767]\n",
      " [4.58679918 0.         7.51917289 8.89373677]\n",
      " [5.47162292 6.85608655 8.60572239 0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 3.98806845  4.70791473  5.5894744   6.36240852]\n",
      " [ 4.53949648  5.57915034  6.8597634   7.97790285]\n",
      " [ 5.2114053   0.          8.08063796 10.        ]\n",
      " [ 6.13535129  7.50291206 10.          0.        ]]\n",
      "Policy\n",
      " [[1 1 2 2]\n",
      " [1 1 2 2]\n",
      " [2 0 1 2]\n",
      " [1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "final_policy = rl.MF.policy\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"Policy\\n\",np.argmax(final_policy,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free Control Sarsa(lambda) (Q-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_lambda\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 1000,  model = \"TD_lambda\", control = True, on_policy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[4.43114744 4.07349403 5.94738645 6.72567215]\n",
      " [4.8200256  5.11001168 6.58105458 7.33392263]\n",
      " [5.48297897 0.         8.31053186 8.66167549]\n",
      " [6.20189836 7.6188342  9.16067796 0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 4.78342161  4.17920904  6.29895301  7.67311587]\n",
      " [ 5.40965105  5.82082145  6.96636495  7.947906  ]\n",
      " [ 6.02885199  0.          8.98376845 10.        ]\n",
      " [ 6.6259179   7.99890536 10.          0.        ]]\n",
      "Policy\n",
      " [[2 1 2 2]\n",
      " [2 1 1 2]\n",
      " [2 0 1 2]\n",
      " [1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "final_policy = rl.MF.policy\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"Policy\\n\",np.argmax(final_policy,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "a = np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  4.,  8., 16.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t = time.time()\n",
    "np.logspace(start=1,stop=len(a),num=len(a),endpoint=True,base=2)/2\n",
    "# print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  4,  8, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t = time.time()\n",
    "np.array([2**i for i in range(len(a))])\n",
    "# print(time.time()-t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

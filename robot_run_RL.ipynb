{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from maze_env import Maze\n",
    "from rl_algo import RL\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def onKeyPress(event):\n",
    "#     is_terminate = False\n",
    "#     if (event.char == \"w\"):\n",
    "#         is_terminate, reward = env.take_action(0, show_animate=True)\n",
    "#     elif (event.char == \"d\"):\n",
    "#         is_terminate, reward = env.take_action(1, show_animate=True)\n",
    "#     elif (event.char == \"s\"):\n",
    "#         is_terminate, reward = env.take_action(2, show_animate=True)\n",
    "#     elif (event.char == \"a\"):\n",
    "#         is_terminate, reward = env.take_action(3, show_animate=True)\n",
    "#     elif (event.char == \"q\"):\n",
    "#         env.destroy()\n",
    "#     if (is_terminate):\n",
    "#         env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4x4, act_space = 4\n"
     ]
    }
   ],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "env = Maze()\n",
    "rl = RL(env)\n",
    "print('{0}x{1}, act_space = {2}'.format(env.MAZE_Limit[0],env.MAZE_Limit[1], len(env.action_space)))\n",
    "# env.bind('<KeyPress>', onKeyPress)\n",
    "# env.mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DP Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 [[ 0.         -6.13796997 -8.35235596 -8.96731567]\n",
      " [-6.13796997 -7.73739624 -8.42782593 -8.35235596]\n",
      " [-8.35235596 -8.42782593 -7.73739624 -6.13796997]\n",
      " [-8.96731567 -8.35235596 -6.13796997  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "rl.DP.iteration(update = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is MC\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 10000,  model = \"MC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -5.24646479 -7.0871772  -7.62785631]\n",
      " [-5.29239044 -6.56155572 -7.22713229 -7.18856686]\n",
      " [-7.1552333  -7.3021403  -6.7505523  -5.394601  ]\n",
      " [-7.64295967 -7.09647788 -5.39378966  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -5.61660451 -7.39513822]\n",
      " [-1.         -5.60056006 -6.97048746 -5.97446889]\n",
      " [-5.87177127 -6.95527475 -5.96531714 -1.        ]\n",
      " [-7.33254942 -5.63556616 -1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "final_policy = rl.MF.policy\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[0],env.MAZE_Limit[1]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[0],env.MAZE_Limit[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free TD(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyper parameters\n",
    "rl.MF.gamma = 0.9 # discount factor\n",
    "rl.MF.lamb = 0.9 # parameter of sarsa\n",
    "rl.MF.alpha = 0.5 # step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_0\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 3000,  model = \"TD_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -4.80102453 -6.7056914  -7.70408986]\n",
      " [-5.64369624 -5.98723472 -6.93271219 -6.37670319]\n",
      " [-7.41974104 -5.74400371 -6.41147944 -5.21596972]\n",
      " [-7.15405123 -6.20877077 -4.49174518  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -5.45906085 -7.21492731]\n",
      " [-1.         -4.17447836 -5.9103544  -4.45704291]\n",
      " [-6.28357657 -5.22819742 -4.36292643 -1.        ]\n",
      " [-5.98782565 -4.93479369 -1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free TD(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_n\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 3000,  model = \"TD_n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -5.69544991 -6.45025642 -7.27810396]\n",
      " [-4.56760864 -6.85018671 -7.09248196 -6.11351027]\n",
      " [-7.57839665 -6.82907906 -5.36809377 -4.07809917]\n",
      " [-6.65534063 -6.18962201 -5.0468156   0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -4.57289185 -6.32054406]\n",
      " [-1.         -4.44142465 -4.54129153 -3.97872722]\n",
      " [-6.26969615 -4.65925913 -3.52315403 -1.        ]\n",
      " [-4.56017177 -5.13775811 -1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free TD(lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_lambda\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 1000,  model = \"TD_lambda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -4.92376575 -7.9851775  -9.02248385]\n",
      " [-4.6368055  -6.08881458 -7.56293002 -7.20709493]\n",
      " [-7.52846959 -5.56751023 -5.18571042 -5.62261753]\n",
      " [-7.18128393 -4.96336066 -4.98836898  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -5.24391312 -8.81504311]\n",
      " [-1.         -4.58781474 -7.00507526 -5.27691819]\n",
      " [-6.14624841 -5.16029347 -2.50664813 -1.        ]\n",
      " [-5.98011812 -2.01803151 -1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free Control MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is MC\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 1000,  model = \"MC\", control = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -1.9535709  -3.49034733 -4.59995441]\n",
      " [-2.05553835 -3.26318374 -4.08023502 -3.78413935]\n",
      " [-3.71834354 -4.05114318 -3.31674566 -2.15035378]\n",
      " [-4.93585946 -3.59006203 -2.06940903  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -2.79636872 -4.3032705 ]\n",
      " [-1.         -2.75411091 -3.91051946 -2.90581575]\n",
      " [-2.91987272 -3.868522   -2.75367413 -1.        ]\n",
      " [-4.29678726 -2.81103894 -1.          0.        ]]\n",
      "Policy\n",
      " [[0 3 3 3]\n",
      " [0 0 2 2]\n",
      " [0 0 2 2]\n",
      " [1 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "final_policy = rl.MF.policy\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"Policy\\n\",np.argmax(final_policy,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model-free Control Sarsa(lambda) (Q-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is TD_lambda\n"
     ]
    }
   ],
   "source": [
    "rl.MF.iteration(n_episode = 1000,  model = \"TD_lambda\", control = True, on_policy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Expect] State-value\n",
      " [[ 0.         -2.02524199 -2.72496139 -4.37597647]\n",
      " [-2.1628405  -3.22611902 -3.68792996 -3.35872205]\n",
      " [-3.15843345 -3.7737247  -2.97198865 -1.87717893]\n",
      " [-4.81644061 -3.05562758 -2.07882591  0.        ]]\n",
      "[Max] Action to State-value\n",
      " [[ 0.         -1.         -1.97413842 -3.43595983]\n",
      " [-1.         -2.48687192 -2.99846897 -2.64736597]\n",
      " [-2.46428569 -3.5438301  -2.45066892 -1.        ]\n",
      " [-4.65416695 -2.30903646 -1.          0.        ]]\n",
      "Policy\n",
      " [[0 3 0 2]\n",
      " [0 0 3 2]\n",
      " [0 3 1 2]\n",
      " [3 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "Avalue = rl.MF.Avalue\n",
    "final_policy = rl.MF.policy\n",
    "print(\"[Expect] State-value\\n\",np.sum(final_policy*Avalue, axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"[Max] Action to State-value\\n\",np.max(Avalue,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))\n",
    "print(\"Policy\\n\",np.argmax(final_policy,axis=0).reshape(env.MAZE_Limit[1],env.MAZE_Limit[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "a = np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  2.,  4.,  8., 16.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t = time.time()\n",
    "np.logspace(start=1,stop=len(a),num=len(a),endpoint=True,base=2)/2\n",
    "# print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  4,  8, 16])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# t = time.time()\n",
    "np.array([2**i for i in range(len(a))])\n",
    "# print(time.time()-t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
